<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
      foundation-math
    </title>

  </head>
  <body>
    <section>
      <h1 style="text-align: center">
        The foundations of artificial intelligence
      </h1>
      <h2>Mathematics (c. 800-present)</h2>
      <p>
        Philosophers staked out most of the important ideas of AI, but to make
        the leap to a formal science required a level of mathematical
        formalization in three main areas: computation, logic, ALGORITHM and
        probability. The notion of expressing a computation as a formal
        algorithm goes back to al-Khowarazmi, an Arab mathematician of the ninth
        century, whose writings also introduced Europe to Arabic numerals and
        algebra. Logic goes back at least to Aristotle, but it was a
        philosophical rather than mathematical subject until George Boole
        (1815-1864) introduced his formal language for making logical inference
        in 1847. Boole's approach was incomplete, but good enough that others
        filled in the gaps. In 1879, Gottlob Frege (1848-1925) produced a logic
        that, except for some notational changes, forms the first-order logic
        that is used today as the most basic knowledge representation system.8
        Alfred Tarski (1902-1983) introduced a theory of reference that shows
        how to relate the objects in a logic to objects in the real world. The
        next step was to determine the limits of what could be done with logic
        and computation.
      </p>
      <p>
        Philosophers staked out most of the important ideas of AI, but to make
        the leap to a formal science required a level of mathematical
        formalization in three main areas: computation, logic, ALGORITHM and
        probability. The notion of expressing a computation as a formal
        algorithm goes back to al-Khowarazmi, an Arab mathematician of the ninth
        century, whose writings also introduced Europe to Arabic numerals and
        algebra. Logic goes back at least to Aristotle, but it was a
        philosophical rather than mathematical subject until George Boole
        (1815-1864) introduced his formal language for making logical inference
        in 1847. Boole's approach was incomplete, but good enough that others
        filled in the gaps. In 1879, Gottlob Frege (1848-1925) produced a logic
        that, except for some notational changes, forms the first-order logic
        that is used today as the most basic knowledge representation system.8
        Alfred Tarski (1902-1983) introduced a theory of reference that shows
        how to relate the objects in a logic to objects in the real world. The
        next step was to determine the limits of what could be done with logic
        and computation. David Hilbert (1862-1943), a great mathematician in his
        own right, is most remembered for the problems he did not solve. In
        1900, he presented a list of 23 problems that he correctly predicted
        would occupy mathematicians for the bulk of the century. The final
        problem asks if there is an algorithm for deciding the truth of any
        logical proposition involving the natural numbers—the famous
        Entscheidungsproblem, or decision problem. Essentially, Hilbert was
        asking if there were fundamental limits to the power of effective proof
        procedures. In 1930, Kurt Godel (1906-1978) showed that there exists an
        effective procedure to prove any true statement in the first-order logic
        of Frege and Russell; but first-order logic could not capture the
        principle of mathematical induction needed to characterize the natural
        numbers. In 1931, he showed that real TNHCEora=METENESS limits do exist.
        His incompleteness theorem showed that in any language expressive enough
        to describe the properties of the natural numbers, there are true
        statements that are undecidable: their truth cannot be established by
        any algorithm. This fundamental result can also be interpreted as
        showing that there are some functions on the integers that cannot be
        represented by an algorithm—that is, they cannot be computed. This
        motivated Alan Turing (1912-1954) to try to characterize exactly which
        functions are capable of being computed. This notion is actually
        slightly problematic, because the notion of a computation or effective
        procedure really cannot be given a formal definition. However, the
        Church-Turing thesis, which states that the Turing machine (Turing,
        1936) is capable of computing any computable function, is generally
        accepted as providing a sufficient definition. Turing also showed that
        there were some functions that no Turing machine can compute. For
        example, no machine can tell in general whether a given program will
        return an answer on a given input, or run forever. Although
        undecidability and noncomputability are important to an understanding of
        com- WTRACTABILITY putation, the notion of intractability has had a much
        greater impact. Roughly speaking, a class of problems is called
        intractable if the time required to solve instances of the class grows
        at least exponentially with the size of the instances. The distinction
        between polynomial and exponential growth in complexity was first
        emphasized in the mid-1960s (Cobham, 1964; Edmonds, 1965). It is
        important because exponential growth means that even moderate-sized in-
        stances cannot be solved in any reasonable time. Therefore, one should
        strive to divide the overall problem of generating intelligent behavior
        into tractable subproblems rather than intractable ones. REDUCTION The
        second important concept in the theory of complexity is reduction, which
        also emerged in the 1960s (Dantzig, 1960; Edmonds, 1962). A reduction is
        a general transformation from one class of problems to another, such
        that solutions to the first class can be found by reducing them to
        problems of the second class and solving the latter problems. NP
        COMPLETENESS How can one recognize an intractable problem? The theory of
        NP-completeness, pioneered by Steven Cook (1971) and Richard Karp
        (1972), provides a method. Cook and Karp showed the existence of large
        classes of canonical combinatorial search and reasoning problems that
        are NP-complete. Any problem class to which an NP-complete problem class
        can be reduced is likely to be intractable. (Although it has not yet
        been proved that NP-complete problems are necessarily intractable, few
        theoreticians believe otherwise.) These results contrast sharply with
        the "Electronic Super-Brain" enthusiasm accompanying the advent of
        computers. Despite the ever-increasing speed of computers, subtlety and
        careful use of resources will characterize intelligent systems. Put
        crudely, the world is an extremely large problem instance! Besides logic
        and computation, the third great contribution of mathematics to AI is
        the j theory of probability. The Italian Gerolamo Cardano (1501-1576)
        first framed the idea of I probability, describing it in terms of the
        possible outcomes of gambling events. Before his time, j the outcomes of
        gambling games were seen as the will of the gods rather than the whim of
        chance, i Probability quickly became an invaluable part of all the
        quantitative sciences, helping to deal with uncertain measurements and
        incomplete theories. Pierre Fermat (1601-1665), Blaise Pascal I
        (1623-1662), James Bernoulli (1654-1705), Pierre Laplace (1749-1827),
        and others advanced j the theory and introduced new statistical methods.
        Bernoulli also framed an alternative view] of probability, as a
        subjective "degree of belief" rather than an objective ratio of
        outcomes.! Subjective probabilities therefore can be updated as new
        evidence is obtained. Thomas Bayes j (1702-1761) proposed a rule for
        updating subjective probabilities in the light of new evidence!
        (published posthumously in 1763). Bayes' rule, and the subsequent field
        of Bayesian analysis,! form the basis of the modern approach to
        uncertain reasoning in AI systems. Debate still rages j between
        supporters of the objective and subjective views of probability, but it
        is not clear if the! difference has great significance for AI. Both
        versions obey the same set of axioms. Savage'sJ (1954) Foundations of
        Statistics gives a good introduction to the field. As with logic, a
        connection must be made between probabilistic reasoning and action.!
        DECISION THEORY Decision theory, pioneered by John Von Neumann and Oskar
        Morgenstern (1944), combines! probability theory with utility theory
        (which provides a formal and complete framework forl specifying the
        preferences of an agent) to give the first general theory that can
        distinguish good! actions from bad ones. Decision theory is the
        mathematical successor to utilitarianism, and] provides the theoretical
        basis for many of the agent designs in this book.
      </p>
    </section>
  </body>
</html>
